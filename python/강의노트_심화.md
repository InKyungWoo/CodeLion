## 같이 푸는 Python

<details>
  <summary>Intro</summary>
  
  - crawler : 기는것, 파충류 -> 기어다니면서 데이터 수집
  - crawling : 크롤러를 사용해 데이터 추출
  
  - 함수 : 나무 조립기, 모듈 : 마을 조립 키트
  - 모듈 설치 `pip install requests`
  - `요청` 클라이언트 -> 서버, `응답` 서버 -> 클라이언트
  ```
  마을 조립 키트.집조립기계(빨간블록)
    return: 집
  ```
  
  ```python
  requests.get(url)
    return: requests.response
  ```
  
```python
import requests

url = "http://www.daum.net"
response = requests.get(url)

print(response)           # -> <Reponse [200]>
  
print(response.text)      # -> html 코드 (텍스트값) 

print(response.url)       # -> https://www.daum.net

print(response.content)   # -> 컨텐츠 내용을 디코드 되지 않은 상태로 출력

print(response.encoding)  # -> 인코딩 반환

print(response.headers)   # -> 헤더 값

print(response.json)      # -> json 파일 출력

print(response.links)     # -> 헤더 링크 dict 객체로 반환

print(response.ok)        # -> 응답 성공 여부 출력

print(response.status_code) # -> 상태코드 출력
  ```

</details>
  
  
<details>
  <summary>실시간 검색어 확인하기</summary>
  
  - `from bs4 import BeautifulSoup` : 띄어쓰기 안됨,  bs4 모듈에서 BeautifulSoup 기능 임포트 (데이터, 파싱방법)
  
  - 데이터 : html 또는 xml
  - Parsing : 데이터를 의미 있는 값으로 변환
  - Parser : 파싱을 도와주는 프로그램 `html.parser`
  
  ```python
import requests
from bs4 import BeautifulSoup

url = "http://www.daum.net/"
response = requests.get(url)
print(type(response.text))      # -> <class 'str'> -> 덩어리 문자열

print(type(BeautifulSoup(response.text, 'html.parser')))    # -> <class 'bs4.BeautifulSoup'>, text를 beautifulsoup 이라는 통에 정리해서 넣어둔 것
  ```
```python
import requests
from bs4 import BeautifulSoup

url = "http://www.daum.net/"
response = requests.get(url)
# print(type(response.text))

soup = BeautifulSoup(response.text, 'html.parser')

print(soup.title)   # -> <title><Daum></title>
```  
```python
import requests
from bs4 import BeautifulSoup

url = "http://www.daum.net/"
response = requests.get(url)
# print(response.text)

soup = BeautifulSoup(response.text, 'html.parser')

print(soup.title)             # -> <title><Daum></title>
print(soup.title.string)      # -> Daum
print(soup.span)              # -> <span class="ico_vert inner_shortcut">주요 서비스 바로가기</span>
print(soup.findAll('span'))   # -> 모든 span 태그 출력
  ```  
- 실시간 검색어 공통점 : <a> 태그, class="link_favorsch @n" <br>
`soup.findAll("a","link_favorsch")` : html 문서에서 모든 a태그 중 class=link_favorscho 인 내용을 가져오는 코드
  <br>
- `open(파일, 모드)` : `r` 읽기 모드 (내용 수정 불가), `w` 쓰기 모드 (신규/덮어쓰기), `a` 추가 모드 (추가, 이어쓰기)
```python
# 다음 실시간 검색어 크롤링
  
from bs4 import BeautifulSoup
import requests
from datetime import datetime

url = "http://www.daum.net/"
response = requests.get(url)
soup = BeautifulSoup(response.text, 'html.parser')
rank = 1

results = soup.findAll('a','link_favorsch')

search_rank_file = open("rankresult.txt","w")     # 파일에 저장, 기존 파일에 이어쓰기 하려면 a 모드로 열기

print(datetime.today().strftime("%Y년 %m월 %d일의 실시간 검색어 순위입니다.\n"))   # 현재 시간

for result in results:
    search_rank_file.write(str(rank)+"위:"+result.get_text()+"\n")
    print(rank,"위 : ",result.get_text(),"\n")
    rank += 1
```  
```python
# 네이버 검색어 크롤링  
  
from bs4 import BeautifulSoup
import requests
from datetime import datetime

# 로봇 아니고 유저야!
headers = {'User-Agent':'Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/63.0.3239.132 Safari/537.36'}
url = "https://datalab.naver.com/keyword/realtimeList.naver?age=20s"
response = requests.get(url,headers=headers)
soup = BeautifulSoup(response.text, 'html.parser')
rank = 1
# span - item_title 네이버 데이터랩 공통점
results = soup.findAll('span','item_title')

print(response.text)

search_rank_file = open("rankresult.txt","a")

print(datetime.today().strftime("%Y년 %m월 %d일의 실시간 검색어 순위입니다.\n"))

for result in results:
    search_rank_file.write(str(rank)+"위:"+result.get_text()+"\n")
    print(rank,"위 : ",result.get_text(),"\n")
    rank += 1
  ```

</details>  
  
<details>
  <summary></summary>
</details>  
  
<details>
  <summary></summary>
</details>    

<details>
  <summary></summary>
</details>    
